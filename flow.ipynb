{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from seaborn_analyzer import regplot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import japanize_matplotlib\n",
    "from openpyxl import load_workbook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import cross_val_score, validation_curve\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj,f)\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実績データ群からDataFrameに変換して.pklファイルとして保存する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str, encoding: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    指定されたファイルからデータフレームを読み込む関数。\n",
    "    \n",
    "    :param file_path: ファイルパス\n",
    "    :param encoding: ファイルのエンコーディング\n",
    "    :return: 読み込まれたデータフレーム\n",
    "    \"\"\"\n",
    "    \n",
    "    # カスタムセパレーターを使用してファイルをDataFrameに読み込み\n",
    "    df = pd.read_csv(file_path, encoding=encoding, header=None, sep='\\s*,\\s*', engine='python')\n",
    "\n",
    "    # 列名を変更\n",
    "    df.columns = ['time', 'value', 'description', 'number']\n",
    "\n",
    "    # time列をDatetime型に変換\n",
    "    df['time'] = pd.to_datetime(df['time'], format='%Y%m%d%H%M')\n",
    "\n",
    "    # インデックスをtime列に設定\n",
    "    df.set_index('time', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_folder(folder_path: str, encoding: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    指定されたフォルダ内のすべてのテキストファイルからデータフレームを読み込む関数。\n",
    "    \n",
    "     既に読み込まれている場合は、そのファイルはスキップし、そのファイル名が出力されます。\n",
    "    \n",
    "     :param folder_path: フォルダパス\n",
    "     :param encoding: ファイルのエンコーディング\n",
    "     :return: 読み込まれたデータフレーム（時系列順）\n",
    "     \"\"\"\n",
    "    \n",
    "     # 結果用の空のデータフレームを作成\n",
    "    result = pd.DataFrame()\n",
    "     \n",
    "      # フォルダ内のすべてのテキストファイルに対して繰り返し処理する。\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.basename(file_path) in result.index:\n",
    "                print(f\"Skipping {file_name} (already loaded)\")\n",
    "                continue\n",
    "\n",
    "            df = read_file(file_path, encoding)\n",
    "            result = pd.concat([result, df])\n",
    "             \n",
    "    return result\n",
    "\n",
    "# 関数を呼び出して結果を表示\n",
    "df = read_folder('/workspaces/data/flow/data/', 'shift_jis')\n",
    "df.to_pickle('/workspaces/data/flow/20220518-20230323.pkl')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .pklファイルから実績元データをロードし、名称振りなおす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/workspaces/data/flow/20220518-20230323.pkl')\n",
    "df_mapping_table = pd.read_csv('/workspaces/data/flow/対応表.csv',header=None, skiprows=1)\n",
    "df_mapping_table.columns = ['number', 'wcs', 'description', '', '', '', '', '']\n",
    "df = df.reset_index().merge(df_mapping_table[['number', 'description']], on='number', how='left', suffixes=('', '_y')).set_index('time')\n",
    "df['description'] = df['description_y']\n",
    "df = df.drop('description_y', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 指定した箇所のnumberでフィルタ、ピボットテーブルで各Number毎の1時間当たりの実績表にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定の複数のnumberを指定します。\n",
    "selected_numbers = {\n",
    "    0:['0-0', '0-1', '0-2'],\n",
    "    1:['1-1', '1-2', '1-3', '1-4', '1-5', '1-6', '1-7', '1-8'],\n",
    "    2:['2-1', '2-2', '2-3', '2-4'],\n",
    "    3:['3-1', '3-2', '3-3', '3-4'],\n",
    "    4:['4-1', '4-2', '4-3'],\n",
    "    5:['5-1', '5-2', '5-3', '5-4'],\n",
    "    6:['6-1', '6-2', '6-3']}\n",
    "\n",
    "# 特定の複数のnumberについてフィルタリングします。\n",
    "df_filtered = df[df['number'].isin(selected_numbers[6])]\n",
    "\n",
    "# time列をdatetime型に変換し、1時間ごとにグループ化してvalueの合計値を計算します。\n",
    "df_result = df_filtered.groupby([pd.Grouper(level='time', freq='H'), 'number'])['value'].sum()\n",
    "df_result = df_result.reset_index().pivot(index='time', columns='number', values='value')\n",
    "df_result.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 週次データ集計および折れ線グラフ化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time列をdatetime型に変換し、週単位にグループ化してvalueの合計値を計算します。\n",
    "df_result_weekly = df_filtered.groupby([pd.Grouper(level='time', freq='W-MON'), 'number'])['value'].sum().reset_index(level=1)\n",
    "df_result_weekly = df_result_weekly.pivot(columns='number', values='value')\n",
    "# 指定したnumber毎に折れ線グラフを作成します。\n",
    "df_result_weekly.plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekly_sum(df, selected_numbers, value_col):\n",
    "    \"\"\"\n",
    "    指定したnumber毎に折れ線グラフを週単位で作成する関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        入力データフレーム\n",
    "    selected_numbers : list of str\n",
    "        指定するnumberのリスト\n",
    "    value_col : str\n",
    "        値を表す列名\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # 特定の複数のnumberについてフィルタリングします。\n",
    "    df_filtered = df[df['number'].isin(selected_numbers)]\n",
    "\n",
    "    # time列をdatetime型に変換し、1時間ごとにグループ化してvalueの合計値を計算します。\n",
    "    df_result = df_filtered.groupby([pd.Grouper(level='time', freq='H'), 'number'])[value_col].sum().reset_index(name=value_col)\n",
    "\n",
    "    # time列をdatetime型に変換し、週単位にグループ化してvalueの合計値を計算します。\n",
    "    df_result_weekly = df_result.groupby([pd.Grouper(key='time', freq='W'), 'number'])[value_col].sum().reset_index(name=value_col)\n",
    "\n",
    "    # 指定したnumber毎に折れ線グラフを作成します。\n",
    "    sns.lineplot(data=df_result_weekly, x='time', y=value_col, hue='number')\n",
    "\n",
    "# 値を表す列名を指定します。\n",
    "value_col = 'value'\n",
    "\n",
    "# 関数を呼び出して折れ線グラフを作成します。\n",
    "plot_weekly_sum(df, selected_numbers[6], value_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_sum(df, selected_numbers, value_col):\n",
    "    \"\"\"\n",
    "    指定したnumber毎に折れ線グラフを月単位で作成する関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        入力データフレーム\n",
    "    selected_numbers : list of str\n",
    "        指定するnumberのリスト\n",
    "    value_col : str\n",
    "        値を表す列名\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # 特定の複数のnumberについてフィルタリングします。\n",
    "    df_filtered = df[df['number'].isin(selected_numbers)]\n",
    "\n",
    "    # time列をdatetime型に変換し、1時間ごとにグループ化してvalueの合計値を計算します。\n",
    "    df_result = df_filtered.groupby([pd.Grouper(level='time', freq='H'), 'number'])[value_col].sum().reset_index(name=value_col)\n",
    "\n",
    "    # time列をdatetime型に変換し、月単位にグループ化してvalueの合計値を計算します。\n",
    "    df_result_monthly = df_result.groupby([pd.Grouper(key='time', freq='M'), 'number'])[value_col].sum().reset_index(name=value_col)\n",
    "\n",
    "    # 指定したnumber毎に折れ線グラフを作成します。\n",
    "    sns.lineplot(data=df_result_monthly, x='time', y=value_col, hue='number')\n",
    "    \n",
    "# 特定の複数のnumberを指定します。\n",
    "selected_numbers = ['6-1', '6-2', '6-3']\n",
    "\n",
    "# 値を表す列名を指定します。\n",
    "value_col = 'value'\n",
    "\n",
    "# 関数を呼び出して折れ線グラフを作成します。\n",
    "plot_monthly_sum(df, selected_numbers, value_col)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selected_numbersで6を指定したときのcolumnの処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outletを追加\n",
    "df_result['outlet'] = df_result.apply(lambda row: row['6-1'] + row['6-3'], axis=1)\n",
    "df_result['all'] = df_result.apply(lambda row: row['6-1'] + row['6-2'] + row['6-3'], axis=1)\n",
    "\n",
    "mapping = ['3F', '2F', '1F', 'outlet', 'all']\n",
    "df_result.columns = mapping\n",
    "df_result.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実績元データをpklファイルへ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result.to_pickle('/workspaces/data/flow/6.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.週間の1時間ごとの実績データグラフ作成用関数   2.各時間帯(1時間ごと)の各系列の実績データの分布グラフ作成関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphs_and_data(result: pd.DataFrame, save_path: str, start_date: str, end_date: str):\n",
    "    # 日付範囲を指定\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # グラフサイズを指定\n",
    "    plt.figure(figsize=(20,5))\n",
    "\n",
    "    # グラフを作成\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        week_end_date = current_date + pd.Timedelta(days=6)\n",
    "        week_data = result.loc[current_date:week_end_date]\n",
    "        ax = week_data.plot(title=f'{current_date.date()} - {week_end_date.date()}', linewidth=1)\n",
    "\n",
    "        # グラフを保存\n",
    "        plt.savefig(f'{save_path}/{current_date.date()}-{week_end_date.date()}.png')\n",
    "\n",
    "        current_date += pd.Timedelta(days=7)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # データフレームをエクセルファイルに保存\n",
    "    result.to_excel(f'{save_path}/result.xlsx')\n",
    "save_graphs_and_data(df_result, '/workspaces/data/flow/graph/', '2022-10-31', '2023-01-09')\n",
    "\n",
    "def visualize_distribution(df: pd.DataFrame, save_path: str, start_date: str, end_date: str):\n",
    "    # 日付範囲を指定\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    df = df.loc[start_date:end_date]\n",
    "    df = df.reset_index()\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    \n",
    "    cols = [col for col in df.columns if col not in ['time', 'hour']]\n",
    "    n_cols = len(cols)\n",
    "    \n",
    "    for hour in range(24):\n",
    "        hour_df = df[df['hour'] == hour]\n",
    "        fig, axes = plt.subplots(n_cols, 1, figsize=(6, 4 * n_cols))\n",
    "        \n",
    "        for i in range(n_cols):\n",
    "            col_name = cols[i]\n",
    "            sns.swarmplot(x='hour', y=col_name, data=hour_df[hour_df[col_name] != 0], ax=axes[i])\n",
    "        \n",
    "        file_name = f'hour_{hour}.png'\n",
    "        file_path = os.path.join(save_path, file_name)\n",
    "        plt.savefig(file_path)\n",
    "visualize_distribution(df_result,'/workspaces/data/flow/graph/hour/', '2022-10-31', '2023-01-09')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### excelファイルをDataFrameへ変換する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_excel_to_df(year: int, month: int, path: str, startarea: str, endarea: str) -> pd.DataFrame:\n",
    "    start_date = datetime(year, month, 1)\n",
    "    end_date = (start_date + timedelta(days=31)).replace(day=1) - timedelta(days=1)\n",
    "    date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    book = load_workbook(path, data_only=True)\n",
    "\n",
    "    df = pd.DataFrame(index=pd.date_range(start=start_date.replace(hour=8), end=end_date.replace(hour=22), freq='H'),\n",
    "                      columns=['ipa', 'sto', 'han', 'div', 'col', 'ent', 'alt'])\n",
    "    df.index.name = 'time'\n",
    "\n",
    "    for date in date_list:\n",
    "        sheet_name = f'{date.day}日'\n",
    "        if sheet_name not in book.sheetnames:\n",
    "            continue\n",
    "\n",
    "        sheet = book[sheet_name]\n",
    "\n",
    "        data = [[cell.value for cell in row] for row in sheet[startarea:endarea]]\n",
    "\n",
    "        col_name = ['ipa', 'sto', 'han', 'div', 'col', 'ent', 'alt']\n",
    "        for i in range(len(col_name)):\n",
    "            df.loc[(df.index.date == date.date()) & (df.index.hour >= 8) & (df.index.hour <= 17), col_name[i]] = data[i][:10]\n",
    "            if data[i][10] is not None and data[i][12] is not None:\n",
    "                df.loc[(df.index.date == date.date()) & (df.index.hour == 18), col_name[i]]=(data[i][10]+data[i][12])/2\n",
    "            elif data[i][10] is not None:\n",
    "                df.loc[(df.index.date == date.date()) & (df.index.hour == 18), col_name[i]]=data[i][10]/2\n",
    "            elif data[i][12] is not None:\n",
    "                df.loc[(df.index.date == date.date()) & (df.index.hour == 18), col_name[i]]=data[i][12]/2    \n",
    "            df.loc[(df.index.date == date.date()) & (df.index.hour >= 19) & (df.index.hour <= 21), col_name[i]] = data[i][13:16]\n",
    "            df.loc[(df.index.date == date.date()) & (df.index.hour == 22), col_name[i]] = data[i][17]\n",
    "            df.loc[(df.index.date == date.date()) & (df.index.hour == 23), col_name[i]] = data[i][18]\n",
    "            df.loc[(df.index.date == date.date() + timedelta(days=1)) & (df.index.hour >= 0) & (df.index.hour <= 6), col_name[i]] = data[i][19:26]\n",
    "\n",
    "    return df.fillna(0)\n",
    "\n",
    "year = 2023\n",
    "month =3\n",
    "path = '/workspaces/data/flow/【QPS】作業計画(3月).xlsx'\n",
    "startarea = 'E27'\n",
    "endarea = 'AD33'\n",
    "\n",
    "df1 = convert_excel_to_df(year, month, path, startarea, endarea)\n",
    "df = pd.read_pickle('/workspaces/data/flow/mhNovDecJanFebMar.pkl')\n",
    "df = df.loc[(df.index < '2023-03-1 00:00:00')]\n",
    "df = pd.concat([df, df1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.index < '2023-03-24 00:00:00'].tail(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('/workspaces/data/flow/mhNovDecJanFebMar.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pklファイルから実績データ読み出し結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow = pd.read_pickle('/workspaces/data/flow/6.pkl')\n",
    "df_flow.head()\n",
    "df_mh = pd.read_pickle('/workspaces/data/flow/mhNovDecJanFebMar.pkl')\n",
    "df_mh.head()\n",
    "# pklファイルからDataFrame読み出して1つのDataFrameにマージ\n",
    "df = df_flow.merge(df_mh, how='outer', on='time').fillna(0)\n",
    "# asfreqメソッドを使用して欠損値をNaNで埋めます。\n",
    "df = df.resample('1H').asfreq()\n",
    "# locメソッドによりdfの期間指定\n",
    "df = df.loc[(df.index >= '2022-11-1 07:00:00') & (df.index < '2023-03-23 23:00:00')]\n",
    "# 最後にfillnaメソッドを使用してNaNを0で埋めます。\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all'].sum()/df[['ipa', 'sto', 'han', 'div', 'col', 'ent', 'alt']].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['3F_efi'] = df.apply(lambda row: row['3F']/ (row['sto'] + row['han'] + row['alt']) if (row['sto'] + row['han'] + row['alt']) != 0 else 0 , axis=1)\n",
    "df['2F_efi'] = df.apply(lambda row: row['2F']/ (row['div'] + row['col']) if (row['div'] + row['col']) != 0 else 0, axis=1)\n",
    "df['1F_efi'] = df.apply(lambda row: row['1F']/ row['ipa'] if row['ipa'] != 0 else 0, axis=1)\n",
    "\n",
    "df['all_efi'] = df.apply(lambda row: row['all']/ (row['sto'] + row['han'] + row['alt'] + row['div'] + row['col'] + row['ipa']) if (row['sto'] + row['han'] + row['alt'] + row['div'] + row['col'] + row['ipa']) != 0 else 0 , axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR（四分位範囲）をもとに外れ値除去(使用しない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spikes(df: pd.DataFrame, select_col: list) -> pd.DataFrame:\n",
    "    for col in select_col:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "selecte_col = ['3F_efi', '2F_efi', '1F_efi']\n",
    "\n",
    "df = remove_spikes(df, select_col=selecte_col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "階差をもとにスパイク除去   \n",
    "指定した列のデータ列からスパイク部分を検知  \n",
    "スパイクがある行は指定列に対応する列(複数可)を0で埋める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spikes_dict(df: pd.DataFrame, select_col: dict, threshold: float) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col, related_cols in select_col.items():\n",
    "        mask = df[col].diff().abs() > threshold\n",
    "        mask |= df[col].diff(+1).abs() > threshold\n",
    "        index = np.where(mask)[0]\n",
    "        for i in index:\n",
    "            if i == 0 or i == len(df) - 1:\n",
    "                continue\n",
    "            # df.iloc[i][col] = (df.iloc[i-1][col] + df.iloc[i+1][col]) / 2\n",
    "            for related_col in related_cols:\n",
    "                # df.iloc[i][related_col] = (df.iloc[i-1][related_col] + df.iloc[i+1][related_col]) / 2\n",
    "                df.iloc[i][related_col] = 0\n",
    "    return df\n",
    "# 入力されたdfのselect_colのkeyで指定された列でスパイク値がある行について、keyに対応する列（複数可）をスパイクの行は0で埋める\n",
    "\n",
    "selecte_col = {\n",
    "    '3F_efi':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F_efi':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F_efi':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all_efi':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\n",
    "\n",
    "# 手入力でのデータがご入力が多いため、個別0埋め\n",
    "df = replace_spikes_dict(df, selecte_col, 350)\n",
    "df['3F_efi'] = df.apply(lambda row: row['3F']/ (row['sto'] + row['han'] + row['alt']) if (row['sto'] + row['han'] + row['alt']) != 0 else 0 , axis=1)\n",
    "df['2F_efi'] = df.apply(lambda row: row['2F']/ (row['div'] + row['col']) if (row['div'] + row['col']) != 0 else 0, axis=1)\n",
    "df['1F_efi'] = df.apply(lambda row: row['1F']/ row['ipa'] if row['ipa'] != 0 else 0, axis=1)\n",
    "df['all_efi'] = df.apply(lambda row: row['all_efi'] if (row['sto'] + row['han'] + row['alt'] + row['div'] + row['col'] + row['ipa']) != 0 else 0 , axis=1)\n",
    "df['3F'] = df.apply(lambda row: 0 if (row['sto'] + row['han'] + row['alt']) == 0 else row['3F'] , axis=1)\n",
    "df['2F'] = df.apply(lambda row: 0 if (row['div'] + row['col']) == 0  else row['2F'] , axis=1)\n",
    "df['1F'] = df.apply(lambda row: 0 if row['ipa'] == 0 else row['1F'], axis=1)\n",
    "df['all'] = df.apply(lambda row: 0 if (row['sto'] + row['han'] + row['alt'] + row['div'] + row['col'] + row['ipa']) == 0 else row['all'] , axis=1)\n",
    "df['outlet'] = df.apply(lambda row: 0 if (row['sto'] + row['han'] + row['alt'] + row['div'] + row['col'] + row['ipa']) == 0 else row['all'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.T == 0).any()]\n",
    "df[df.index >= '2022-11-05 0:00:00'].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df.loc[(df.index >= '2023-02-01 00:00:00') & (df.index <= '2023-04-1 00:00:00')]\n",
    "plt.figure(figsize=(50, 5))\n",
    "df_t = df_t.reset_index().melt('time', var_name='cols', value_name='vals')\n",
    "df_t = df_t[df_t['cols'].isin(['3F_efi', '2F_efi', '1F_efi', 'all_efi'])]\n",
    "sns.lineplot(x='time', y='vals', hue='cols', data=df_t, linewidth=1)\n",
    "\n",
    "# 横軸の設定\n",
    "ax = plt.gca()\n",
    "hours = mdates.HourLocator(interval=4)\n",
    "h_fmt = mdates.DateFormatter('%H時')\n",
    "ax.xaxis.set_major_locator(hours)\n",
    "ax.xaxis.set_major_formatter(h_fmt)\n",
    "\n",
    "# 横軸ラベルの文字サイズ設定\n",
    "ax.tick_params(axis='x', labelsize=6)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig('/workspaces/data/flow/plot.jpg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理済みデータpklファイルロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('/workspaces/data/flow/preprocessed_dataset.pkl')\n",
    "df = pd.read_pickle('/workspaces/data/flow/preprocessed_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "print(len(df[(df['alt'] < num)].apply(lambda col:col[col != 0])['1F_efi']))\n",
    "df_t = df[(df['alt'] < num)].apply(lambda col:col[col != 0])['1F_efi'].dropna()\n",
    "print(len(df[((df['alt'] > num))].apply(lambda col:col[col != 0])['1F_efi']))\n",
    "df_tt = df[((df['alt'] > num))].apply(lambda col:col[col != 0])['1F_efi'].dropna()\n",
    "\n",
    "print(df_t.describe())\n",
    "print(df_tt.describe())\n",
    "\n",
    "# ウェルチのt検定\n",
    "t_stat, p_value = stats.ttest_ind(df_t, df_tt, equal_var=False)\n",
    "print(\"Welch's t-test: p-value =\", p_value)\n",
    "\n",
    "# マン・ホイットニーのU検定\n",
    "u_stat, p_value = stats.mannwhitneyu(df_t, df_tt, alternative='two-sided')\n",
    "print(\"Mann-Whitney U test: p-value =\", p_value)\n",
    "# ヒストグラムとKDEプロットを重ねて描画\n",
    "sns.histplot(df_t, kde=True, color='blue', alpha=0.5, label='< num')\n",
    "sns.histplot(df_tt, kde=True, color='red', alpha=0.5, label='> num')\n",
    "\n",
    "# グラフのタイトルと凡例を追加\n",
    "plt.title('Distribution of Group 1 and Group 2')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ロバスト回帰分析(フーバー回帰)を実行し、各種プロットする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_robust_regression_and_residual_analysis(data, var_dict, save_path, epsilon=1.35):\n",
    "    # 入力検証\n",
    "    validate_input(var_dict, data)\n",
    "\n",
    "    # 指定されたパスにフォルダがなければ作成\n",
    "    create_folder(save_path)\n",
    "\n",
    "    output = {}\n",
    "    for y_var, x_vars in var_dict.items():\n",
    "        # 説明変数と目的変数の設定\n",
    "        X = data[x_vars]\n",
    "        y = data[y_var]\n",
    "\n",
    "        # 回帰分析の実行\n",
    "        model = HuberRegressor(epsilon=epsilon)\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        output[y_var] = model\n",
    "        \n",
    "        print(f\"Results for {y_var}:\\n\")\n",
    "        print('Coefficients: \\n', model.coef_)\n",
    "\n",
    "        coefs_df = pd.DataFrame({'variable': x_vars,'coefficient': model.coef_})\n",
    "        coefs_df.to_csv(os.path.join(save_path,f'{y_var}_coefficients_robustlinear_{epsilon}.csv'),index=False)\n",
    "        \n",
    "        # 残差プロット\n",
    "        residuals = y - model.predict(X)\n",
    "        \n",
    "        # 標準化残差プロット\n",
    "        standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "        plt.scatter(model.predict(X), standardized_residuals)\n",
    "        plt.xlabel('Fitted values')\n",
    "        plt.ylabel('Standardized residuals')\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_standardized_residuals_robustlinear_{epsilon}.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Q-Qプロット\n",
    "        stats.probplot(standardized_residuals, dist=\"norm\", plot=plt)\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_qqplot_robustlinear_{epsilon}.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # 残差 vs. 説明変数プロット \n",
    "        fig, ax = plt.subplots(1,len(x_vars), figsize=(30, 5))\n",
    "        for i in range(len(x_vars)):\n",
    "            if len(x_vars) > 1:\n",
    "                ax[i].scatter(data[x_vars[i]], residuals)\n",
    "                ax[i].set_xlabel(x_vars[i])\n",
    "                ax[i].set_ylabel('Residuals')\n",
    "            else:\n",
    "                ax.scatter(data[x_vars[i]], residuals)\n",
    "                ax.set_xlabel(x_vars[i])\n",
    "                ax.set_ylabel('Residuals')\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_residuals_vs_explanatory_robustlinear_{epsilon}.png'))\n",
    "        plt.show()\n",
    "    return output\n",
    "\n",
    "def validate_input(var_dict,data):\n",
    "    if not isinstance(var_dict, dict):\n",
    "        raise ValueError(\"var_dict must be a dictionary\")\n",
    "    for y_var,x_vars in var_dict.items():\n",
    "        if not isinstance(y_var,str) or not isinstance(x_vars,list):\n",
    "            raise ValueError(\"var_dict keys must be strings and values must be lists\")\n",
    "        if y_var not in data.columns:\n",
    "            raise ValueError(f\"{y_var} is not a column in the data\")\n",
    "        for x_var in x_vars:\n",
    "            if x_var not in data.columns:\n",
    "                raise ValueError(f\"{x_var} is not a column in the data\")\n",
    "\n",
    "def create_folder(save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "# データの読み込み\n",
    "data = df.loc[(df.index >= '2023-02-01 00:00:00') & (df.index <= '2023-03-1 00:00:00')]\n",
    "\n",
    "var_dict = {\n",
    "    '3F':['sto', 'han', 'alt'],\n",
    "    '2F':['div', 'col'],\n",
    "    '1F':['ipa'],\n",
    "    'all':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\n",
    "\n",
    "save_path = \"/workspaces/data/flow/robustlinearFeb_scikit\"\n",
    "\n",
    "run_robust_regression_and_residual_analysis(data, var_dict, save_path, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sm_regression_and_residual_analysis(data, var_dict, save_path):\n",
    "    # 入力検証\n",
    "    validate_input(var_dict,data)\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    # 指定されたパスにフォルダがなければ作成\n",
    "    create_folder(save_path)\n",
    "\n",
    "    for y_var, x_vars in var_dict.items():\n",
    "        # 説明変数と目的変数の設定\n",
    "        X = data[x_vars]\n",
    "        y = data[y_var]\n",
    "\n",
    "        # 回帰分析の実行\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y,X)\n",
    "        output[y_var] = model\n",
    "        results = model.fit()\n",
    "\n",
    "        print(f\"Results for {y_var}:\\n\")\n",
    "        print(results.summary())\n",
    "\n",
    "        # 回帰係数の優位性の確認\n",
    "        print(f\"\\nP-values for {y_var}:\\n\")\n",
    "        print(results.pvalues)\n",
    "\n",
    "        # 回帰係数をcsvファイルとして保存\n",
    "        save_coefficients(results,y_var,save_path)\n",
    "\n",
    "        # 標準化残差プロット\n",
    "        plot_standardized_residuals(results,y_var,save_path)\n",
    "\n",
    "        # Q-Qプロット\n",
    "        plot_qqplot(results,y_var,save_path)\n",
    "\n",
    "        # 残差 vs. 説明変数プロット\n",
    "        plot_residuals_vs_explanatory(data,x_vars,y_var,results,save_path,output)\n",
    "            \n",
    "    return output\n",
    "\n",
    "def validate_input(var_dict,data):\n",
    "    if not isinstance(var_dict, dict):\n",
    "        raise ValueError(\"var_dict must be a dictionary\")\n",
    "    for y_var,x_vars in var_dict.items():\n",
    "        if not isinstance(y_var,str) or not isinstance(x_vars,list):\n",
    "            raise ValueError(\"var_dict keys must be strings and values must be lists\")\n",
    "        if y_var not in data.columns:\n",
    "            raise ValueError(f\"{y_var} is not a column in the data\")\n",
    "        for x_var in x_vars:\n",
    "            if x_var not in data.columns:\n",
    "                raise ValueError(f\"{x_var} is not a column in the data\")\n",
    "\n",
    "def create_folder(save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "def save_coefficients(results,y_var,save_path):\n",
    "    coefs_df = pd.DataFrame({'variable':results.params.index,'coefficient':results.params.values})\n",
    "    coefs_df.to_csv(os.path.join(save_path,f'{y_var}_coefficients.csv'),index=False)\n",
    "\n",
    "def plot_standardized_residuals(results,y_var,save_path):\n",
    "    plt.scatter(results.fittedvalues, results.get_influence().resid_studentized_internal)\n",
    "    plt.xlabel('Fitted values')\n",
    "    plt.ylabel('Standardized residuals')\n",
    "    plt.savefig(os.path.join(save_path,f'{y_var}_standardized_residuals.png'))\n",
    "    plt.show()\n",
    "\n",
    "def plot_qqplot(results,y_var,save_path):\n",
    "    sm.qqplot(results.get_influence().resid_studentized_internal,line='45')\n",
    "    plt.savefig(os.path.join(save_path,f'{y_var}_qqplot.png'))\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals_vs_explanatory(data,x_vars,y_var,results,save_path,output):\n",
    "    fig, ax = plt.subplots(1,len(x_vars), figsize=(20, 5))\n",
    "    for i in range(len(x_vars)):\n",
    "        if len(x_vars) > 1:\n",
    "            ax[i].scatter(data[x_vars[i]], results.resid)\n",
    "            ax[i].set_xlabel(x_vars[i])\n",
    "            ax[i].set_ylabel('Residuals')\n",
    "        else:\n",
    "            ax.scatter(data[x_vars[i]], results.resid)\n",
    "            ax.set_xlabel(x_vars[i])\n",
    "            ax.set_ylabel('Residuals')\n",
    "    plt.savefig(os.path.join(save_path,f'{y_var}_residuals_vs_explanatory.png'))\n",
    "    plt.show()\n",
    "\n",
    "# データの読み込み\n",
    "data = df.loc[(df.index >= '2023-02-01 00:00:00') & (df.index <= '2023-03-1 00:00:00')]\n",
    "\n",
    "var_dict = {\n",
    "    '3F':['sto', 'han', 'alt'],\n",
    "    '2F':['div', 'col'],\n",
    "    '1F':['ipa'],\n",
    "    'all':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\n",
    "\n",
    "save_path = \"/workspaces/data/flow/linearFeb_sm\"\n",
    "\n",
    "# 重回帰分析と残差診断の実行\n",
    "models = run_sm_regression_and_residual_analysis(data, var_dict, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 線形重回帰分析(scikit_learn)を実行し、各種プロットする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scikit_regression_and_residual_analysis(data, var_dict, save_path):\n",
    "    # 入力検証\n",
    "    validate_input(var_dict,data)\n",
    "\n",
    "    output = {}\n",
    "\n",
    "    # 指定されたパスにフォルダがなければ作成\n",
    "    create_folder(save_path)\n",
    "\n",
    "    for y_var, x_vars in var_dict.items():\n",
    "        # 説明変数と目的変数の設定\n",
    "        X = data[x_vars]\n",
    "        y = data[y_var]\n",
    "\n",
    "        # 回帰分析の実行\n",
    "        model = LinearRegression()\n",
    "        model.fit(X,y)\n",
    "        \n",
    "        output[y_var] = model\n",
    "\n",
    "        # 結果の表示\n",
    "        print(f\"Results for {y_var}:\\n\")\n",
    "        print('Coefficients: \\n', model.coef_)\n",
    "        \n",
    "        coefs_df = pd.DataFrame({'variable': x_vars,'coefficient': model.coef_})\n",
    "        coefs_df.to_csv(os.path.join(save_path,f'{y_var}_coefficients_linear.csv'),index=False)\n",
    "\n",
    "        # 残差プロット\n",
    "        residuals = y - model.predict(X)\n",
    "        \n",
    "        # 標準化残差プロット\n",
    "        standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "        plt.scatter(model.predict(X), standardized_residuals)\n",
    "        plt.xlabel('Fitted values')\n",
    "        plt.ylabel('Standardized residuals')\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_standardized_residuals_linear.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Q-Qプロット\n",
    "        stats.probplot(standardized_residuals, dist=\"norm\", plot=plt)\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_qqplot_linear.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # 残差 vs. 説明変数プロット\n",
    "        fig, ax = plt.subplots(1,len(x_vars), figsize=(20, 5))\n",
    "        for i in range(len(x_vars)):\n",
    "            if len(x_vars) > 1:\n",
    "                ax[i].scatter(data[x_vars[i]], residuals)\n",
    "                ax[i].set_xlabel(x_vars[i])\n",
    "                ax[i].set_ylabel('Residuals')\n",
    "            else:\n",
    "                ax.scatter(data[x_vars[i]], residuals)\n",
    "                ax.set_xlabel(x_vars[i])\n",
    "                ax.set_ylabel('Residuals')\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_residuals_vs_explanatory_linear.png'))\n",
    "        plt.show()\n",
    "\n",
    "    return output\n",
    "\n",
    "def validate_input(var_dict,data):\n",
    "    if not isinstance(var_dict, dict):\n",
    "        raise ValueError(\"var_dict must be a dictionary\")\n",
    "    for y_var,x_vars in var_dict.items():\n",
    "        if not isinstance(y_var,str) or not isinstance(x_vars,list):\n",
    "            raise ValueError(\"var_dict keys must be strings and values must be lists\")\n",
    "        if y_var not in data.columns:\n",
    "            raise ValueError(f\"{y_var} is not a column in the data\")\n",
    "        for x_var in x_vars:\n",
    "            if x_var not in data.columns:\n",
    "                raise ValueError(f\"{x_var} is not a column in the data\")\n",
    "\n",
    "def create_folder(save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "# データの読み込み\n",
    "data = df.loc[(df.index >= '2023-02-01 00:00:00') & (df.index <= '2023-03-1 00:00:00')]\n",
    "\n",
    "\"\"\"var_dict = {\n",
    "    '3F':['sto', 'han', 'alt'],\n",
    "    '2F':['div', 'col'],\n",
    "    '1F':['ipa'],\n",
    "    'all':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\"\"\"\n",
    "\n",
    "var_dict = {\n",
    "    '3F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\n",
    "\n",
    "save_path = \"/workspaces/data/flow/linearFeb_scikit\"\n",
    "\n",
    "# 重回帰分析と残差診断の実行\n",
    "models = run_scikit_regression_and_residual_analysis(data, var_dict, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_regression_model(model_output, validation_data):\n",
    "    \"\"\"\n",
    "    model_output: 辞書。キーは従属変数名であり、値はstatsmodels OLSモデルオブジェクトです。\n",
    "    validation_data: 検証用のpandas DataFrame。\n",
    "    \"\"\"\n",
    "    for y_var, model in model_output.items():\n",
    "        X = validation_data[model.exog_names[1:]]\n",
    "        X = sm.add_constant(X)\n",
    "        y_pred = model.predict(exog=X, params=model.fit().params)\n",
    "        print(f\"Predictions for {y_var}:\\n\")\n",
    "        print(y_pred)\n",
    "\n",
    "# データの読み込み\n",
    "data = df.loc[(df.index >= '2023-01-01 00:00:00') & (df.index <= '2023-02-1 00:00:00')]\n",
    "\n",
    "var_dict = {\n",
    "    '3F':['sto', 'han', 'alt'],\n",
    "    '2F':['div', 'col'],\n",
    "    '1F':['ipa'],\n",
    "    'all':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\n",
    "\n",
    "validate_regression_model(models, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ランダムフォレストによる回帰分析を実行し、各種プロットしたグラフ画像を指定フォルダに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tree_regression_and_residual_analysis(train_df, test_df, var_dict, param_grid, save_path):\n",
    "    \"\"\"\n",
    "    説明変数と目的変数を指定して、複数の決定木回帰モデルを構築し、予測値を計算する関数。\n",
    "    :param df: データフレーム。説明変数と目的変数が含まれる。\n",
    "    :type df: pandas.DataFrame\n",
    "    :param var_dict: 説明変数と目的変数の辞書。キーが目的変数名で、値が説明変数名のリスト。\n",
    "    :type var_dict: dict of str to list of str\n",
    "    :param save_path: グラフを保存するフォルダのパス。\n",
    "    :type save_path: str\n",
    "    :return: 構築したモデルと予測値の辞書\n",
    "    :rtype: tuple of dict\n",
    "    \"\"\"\n",
    "    # 入力検証\n",
    "    validate_input(var_dict,train_df)\n",
    "    validate_input(var_dict,test_df)\n",
    "\n",
    "    # 指定されたパスにフォルダがなければ作成\n",
    "    create_folder(save_path)\n",
    "\n",
    "    models = {}\n",
    "    train_y_preds = {}\n",
    "    test_y_preds = {}\n",
    "\n",
    "    # 結果を格納するDataFrame作成\n",
    "    result_df = pd.DataFrame(columns=['y_var', 'train_mse', 'train_mae', 'train_r2', 'test_mse', 'test_mae', 'test_r2'])\n",
    "\n",
    "    # 指定されたフォルダが存在しない場合は作成する。\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for y_var, x_vars in var_dict.items():\n",
    "        # モデルインスタンス作成\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "        # グリッドサーチ実行\n",
    "        grid_search = GridSearchCV(estimator=model,\n",
    "                                   param_grid=param_grid,\n",
    "                                   cv=3,\n",
    "                                   n_jobs=-1)\n",
    "        grid_search.fit(train_df[x_vars], train_df[y_var])\n",
    "\n",
    "        # 最適なパラメータでモデル再学習\n",
    "        best_model = grid_search.best_estimator_\n",
    "        models[y_var] = best_model\n",
    "\n",
    "\n",
    "        # 学習用データでの予測と評価指標計算\n",
    "        train_y_pred = best_model.predict(train_df[x_vars])\n",
    "        train_y_preds[y_var] = train_y_pred\n",
    "        train_mse = mean_squared_error(train_df[y_var], train_y_pred)\n",
    "        train_mae = mean_absolute_error(train_df[y_var], train_y_pred)\n",
    "        train_r2 = r2_score(train_df[y_var], train_y_pred)\n",
    "        \n",
    "        # 検証用データでの予測と評価指標計算\n",
    "        test_y_pred = best_model.predict(test_df[x_vars])\n",
    "        test_y_preds[y_var] = test_y_pred\n",
    "        test_mse = mean_squared_error(test_df[y_var], test_y_pred)\n",
    "        test_mae = mean_absolute_error(test_df[y_var], test_y_pred)\n",
    "        test_r2 = r2_score(test_df[y_var], test_y_pred)\n",
    "\n",
    "        # 結果のサマリ表示\n",
    "        \"\"\"\n",
    "        print(f'{y_var} の学習用MSE:', train_mse)\n",
    "        print(f'{y_var} の学習用MAE:', train_mae)\n",
    "        print(f'{y_var} の学習用決定係数:', train_r2)\n",
    "\n",
    "        print(f'{y_var} の検証用MSE:', test_mse)\n",
    "        print(f'{y_var} の検証用MAE:', test_mae)\n",
    "        print(f'{y_var} の検証用決定係数:', test_r2)\"\"\"\n",
    "        \n",
    "        # 結果をDataFrameに追加\n",
    "        result_df.loc[len(result_df)] = [y_var, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2]\n",
    "\n",
    "        # 結果をCSVファイルに保存\n",
    "        result_file_path = os.path.join(save_path, 'result.csv')\n",
    "        result_df.to_csv(result_file_path,index=False)\n",
    "\n",
    "        # 残差計算\n",
    "        residuals = train_df[y_var] - train_y_pred\n",
    "\n",
    "        # 標準化残差計算\n",
    "        standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "\n",
    "        # グラフのプロット部\n",
    "        \"\"\"\n",
    "        # 残差プロット表示および保存\n",
    "        plt.scatter(train_y_pred, residuals)\n",
    "        plt.xlabel('予測値')\n",
    "        plt.ylabel('残差')\n",
    "        plt.title(f'{y_var} の予測値 vs 残差')\n",
    "        plt.savefig(os.path.join(save_path, f'{y_var}_residual_plot_tree_maxdepth{max_depth}.png'))\n",
    "        plt.show()\n",
    "\n",
    "        # 標準化残差プロット表示および保存\n",
    "        plt.scatter(train_y_pred, standardized_residuals)\n",
    "        plt.xlabel('予測値')\n",
    "        plt.ylabel('標準化残差')\n",
    "        plt.title(f'{y_var} の予測値 vs 標準化残差')\n",
    "        plt.savefig(os.path.join(save_path, f'{y_var}_standardized_residual_plot_tree_maxdepth{max_depth}.png'))\n",
    "        plt.show()\n",
    "\n",
    "        # Q-Qプロット表示および保存\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "        plt.title(f'{y_var} のQ-Qプロット')\n",
    "        plt.savefig(os.path.join(save_path, f'{y_var}_qq_plot_tree_maxdepth{max_depth}.png'))\n",
    "        plt.show()\n",
    "\n",
    "        # 残差 vs. 説明変数プロット\n",
    "        fig, ax = plt.subplots(1,len(x_vars), figsize=(20, 5))\n",
    "        for i in range(len(x_vars)):\n",
    "            if len(x_vars) > 1:\n",
    "                ax[i].scatter(df[x_vars[i]], residuals)\n",
    "                ax[i].set_xlabel(x_vars[i])\n",
    "                ax[i].set_ylabel('Residuals')\n",
    "            else:\n",
    "                ax.scatter(df[x_vars[i]], residuals)\n",
    "                ax.set_xlabel(x_vars[i])\n",
    "                ax.set_ylabel('Residuals')\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_residuals_vs_explanatory_tree_maxdepth{max_depth}.png'))\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "    return models\n",
    "\n",
    "def validate_input(var_dict,data):\n",
    "    if not isinstance(var_dict, dict):\n",
    "        raise ValueError(\"var_dict must be a dictionary\")\n",
    "    for y_var,x_vars in var_dict.items():\n",
    "        if not isinstance(y_var,str) or not isinstance(x_vars,list):\n",
    "            raise ValueError(\"var_dict keys must be strings and values must be lists\")\n",
    "        if y_var not in data.columns:\n",
    "            raise ValueError(f\"{y_var} is not a column in the data\")\n",
    "        for x_var in x_vars:\n",
    "            if x_var not in data.columns:\n",
    "                raise ValueError(f\"{x_var} is not a column in the data\")\n",
    "\n",
    "def create_folder(save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "# データの読み込み\n",
    "train_df = df.loc[(df.index >= '2022-12-01 00:00:00') & (df.index < '2023-02-1 00:00:00')]\n",
    "test_df = df.loc[(df.index >= '2023-02-01 00:00:00') & (df.index < '2023-03-1 00:00:00')]\n",
    "\n",
    "var_dict = {\n",
    "    '3F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\n",
    "\n",
    "# グリッドサーチ用パラメータ設定\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 80],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [0.02]\n",
    "}\n",
    "\n",
    "save_path = \"/workspaces/data/flow/RFtrain1201test0233\"\n",
    "\n",
    "models = run_tree_regression_and_residual_analysis(train_df, test_df, var_dict, param_grid, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(df, var_dict, save_path):\n",
    "    kf = KFold(n_splits=5)\n",
    "    models = {}\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        train_df = df.iloc[train_index]\n",
    "        test_df = df.iloc[test_index]\n",
    "        models.update(run_tree_regression_and_residual_analysis(train_df, test_df, var_dict, save_path))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote averageが下位50%だったらlow, 上位50%だったらhigh\n",
    "sns.pairplot(df[['ipa', 'sto', 'han', 'div', 'col', 'alt']],\n",
    "             plot_kws={'alpha':0.3},\n",
    "             #diag_kind='hist'\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 勾配ブースティング(XGBRegressor)のハイパーパラメータのあたり付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_df = df.loc[(df.index >= '2022-12-01 00:00:00') & (df.index < '2023-02-1 00:00:00')].reset_index()\n",
    "test_df = df.loc[(df.index >= '2023-02-01 00:00:00') & (df.index < '2023-03-1 00:00:00')].reset_index()\n",
    "\n",
    "USE_EXPLANATORY = ['ipa', 'sto', 'han', 'alt']\n",
    "OBJECTIVE_VARIALBLE = '3F'\n",
    "\n",
    "X = train_df[USE_EXPLANATORY].values\n",
    "Y = train_df[OBJECTIVE_VARIALBLE]\n",
    "\n",
    "#  乱数シード\n",
    "seed = 23\n",
    "# モデル作成\n",
    "model = XGBRegressor(booster='gbtree', objective='reg:absoluteerror', random_state=seed, n_estimators=10000)\n",
    "# 学習時fitパラメータ指定\n",
    "fit_params = {'verbose': False,\n",
    "                'early_stopping_rounds': 15,\n",
    "                'eval_metric': 'mae',\n",
    "                'eval_set': [([X, Y])]}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\"\"\"\n",
    "regplot.regression_heat_plot(model, USE_EXPLANATORY, OBJECTIVE_VARIALBLE, train_df,\n",
    "                             pair_sigmarange = 0.5, rounddigit_x1=3, rounddigit_x2=3,\n",
    "                             cv=cv, display_cv_indices=0,\n",
    "                             fit_params=fit_params)\n",
    "\"\"\"\n",
    "\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "scores = cross_val_score(model, X, Y, cv=cv, scoring=scoring, n_jobs=-1, fit_params=fit_params)\n",
    "print(f'scores={scores}')\n",
    "print(f'average_score={np.mean(scores)}')\n",
    "\n",
    "cv_params = {'subsample': [0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'colsample_bytree': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'reg_alpha': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "             'reg_lambda': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "             'learning_rate': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "             'min_child_weight': [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "             'gamma': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0]\n",
    "             }\n",
    "param_scales = {'subsample': 'linear',\n",
    "                'colsample_bytree': 'linear',\n",
    "                'reg_alpha': 'log',\n",
    "                'reg_lambda': 'log',\n",
    "                'learning_rate': 'log',\n",
    "                'min_child_weight': 'linear',\n",
    "                'max_depth': 'linear',\n",
    "                'gamma': 'log'\n",
    "                }\n",
    "\n",
    "# 検証曲線のプロット（パラメータ毎にプロット）\n",
    "for i, (k, v) in enumerate(cv_params.items()):\n",
    "    train_scores, valid_scores = validation_curve(estimator=model,\n",
    "                                                  X=X, y=Y,\n",
    "                                                  param_name=k,\n",
    "                                                  param_range=v,\n",
    "                                                  fit_params=fit_params,\n",
    "                                                  cv=cv, scoring=scoring,\n",
    "                                                  n_jobs=-1)\n",
    "    # 学習データに対するスコアの平均±標準偏差を算出\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std  = np.std(train_scores, axis=1)\n",
    "    train_center = train_mean\n",
    "    train_high = train_mean + train_std\n",
    "    train_low = train_mean - train_std\n",
    "    # テストデータに対するスコアの平均±標準偏差を算出\n",
    "    valid_mean = np.mean(valid_scores, axis=1)\n",
    "    valid_std  = np.std(valid_scores, axis=1)\n",
    "    valid_center = valid_mean\n",
    "    valid_high = valid_mean + valid_std\n",
    "    valid_low = valid_mean - valid_std\n",
    "    # training_scoresをプロット\n",
    "    plt.plot(v, train_center, color='blue', marker='o', markersize=5, label='training score')\n",
    "    plt.fill_between(v, train_high, train_low, alpha=0.15, color='blue')\n",
    "    # validation_scoresをプロット\n",
    "    plt.plot(v, valid_center, color='green', linestyle='--', marker='o', markersize=5, label='validation score')\n",
    "    plt.fill_between(v, valid_high, valid_low, alpha=0.15, color='green')\n",
    "    # スケールをparam_scalesに合わせて変更\n",
    "    plt.xscale(param_scales[k])\n",
    "    # 軸ラベルおよび凡例の指定\n",
    "    plt.xlabel(k)  # パラメータ名を横軸ラベルに\n",
    "    plt.ylabel(scoring)  # スコア名を縦軸ラベルに\n",
    "    plt.legend(loc='lower right')  # 凡例\n",
    "    # グラフを描画\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配ブースティングのための関数群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_input(target_to_features, data):\n",
    "    \"\"\"\n",
    "    入力変数とデータ構造を検証します。\n",
    "    Args:\n",
    "        target_to_features (dict): 目的変数と対応する説明変数のディクショナリ。\n",
    "        data (pd.DataFrame): 入力データ。\n",
    "    Raises:\n",
    "        ValueError: 入力が無効な場合。\n",
    "    \"\"\"\n",
    "    if not isinstance(target_to_features, dict):\n",
    "        raise ValueError(\"target_to_features must be a dictionary\")\n",
    "    for target, features in target_to_features.items():\n",
    "        if not isinstance(target, str) or not isinstance(features, list):\n",
    "            raise ValueError(\"target_to_features keys must be strings and values must be lists\")\n",
    "        if target not in data.columns:\n",
    "            raise ValueError(f\"{target} is not a column in the data\")\n",
    "        for feature in features:\n",
    "            if feature not in data.columns:\n",
    "                raise ValueError(f\"{feature} is not a column in the data\")\n",
    "\n",
    "def create_directory_if_not_exists(directory_path):\n",
    "    \"\"\"\n",
    "    指定されたパスにフォルダが存在しない場合、フォルダを作成します。\n",
    "    Args:\n",
    "        directory_path (str): フォルダを作成するパス。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "def add_hour_column(df):\n",
    "    \"\"\"\n",
    "    'time'列から時刻を抽出し、新しい列としてDataFrameに追加します。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 入力データ。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 新しい 'hour' 列が追加されたDataFrame。\n",
    "    \"\"\"\n",
    "    df['hour'] = pd.to_datetime(df['time']).dt.hour\n",
    "    return df\n",
    "\n",
    "def one_hot_encode_hours(df):\n",
    "    \"\"\"\n",
    "    'hour' 列をワンホットエンコードし、変更されたDataFrameを返します。\n",
    "    Args:\n",
    "        df (pd.DataFrame): 入力データ。\n",
    "    Returns:\n",
    "        pd.DataFrame: ワンホットエンコードされた 'hour' 列を持つDataFrame。\n",
    "    \"\"\"\n",
    "    df = pd.get_dummies(df, columns=['hour'], prefix='hour', drop_first=True)\n",
    "    return df\n",
    "\n",
    "def load_hyperparameters_from_csv(params_directory, target):\n",
    "    \"\"\"\n",
    "    与えられた目的変数のCSVファイルからハイパーパラメータを読み込みます。\n",
    "    Args:\n",
    "        params_directory (str): ハイパーパラメータを含むCSVファイルがあるフォルダへのパス。\n",
    "        target (str): 目的変数名。\n",
    "    Returns:\n",
    "        dict: 目的変数のハイパーパラメータを含むディクショナリ。\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(params_directory, f'best_params_{target}.csv')\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    # Convert values to appropriate types\n",
    "    hyperparams = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if index in ['max_depth', 'min_child_weight']:\n",
    "            hyperparams[index] = int(row['value'])\n",
    "        elif index in ['colsample_bytree', 'learning_ration', 'subsample']:\n",
    "            hyperparams[index] = float(row['value'])\n",
    "    return hyperparams\n",
    "\n",
    "def plot_actual_vs_predicted_and_residuals(train_df, test_df, target, features, train_y_pred, test_y_pred, residuals, save_directory, eval_metric):\n",
    "    \"\"\"\n",
    "    実際の値と予測値のグラフおよび残差グラフをプロットし、保存する関数。\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): 学習用データセット。\n",
    "        test_df (pd.DataFrame): 検証用データセット。\n",
    "        target (str): 目的変数。\n",
    "        features (list): 説明変数のリスト。\n",
    "        train_y_pred (np.array): 学習用データの予測値。\n",
    "        test_y_pred (np.array): 検証用データの予測値。\n",
    "        residuals (np.array): 学習用データの残差。\n",
    "        save_directory (str): 結果を保存するディレクトリへのパス。\n",
    "        eval_metric (str): 評価指標。\n",
    "    \"\"\"\n",
    "    # 観測値、予測値 vs. time\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(50, 16))\n",
    "    axs[0].plot(train_df['time'], train_df[target], label='Actual')\n",
    "    axs[0].plot(train_df['time'], train_y_pred, label='Predicted')\n",
    "    axs[0].set_title('Train Data')\n",
    "    axs[0].set_xlabel('Time')\n",
    "    axs[0].set_ylabel(target)\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(test_df['time'], test_df[target], label='Actual')\n",
    "    axs[1].plot(test_df['time'], test_y_pred, label='Predicted')\n",
    "    axs[1].set_title('Test Data')\n",
    "    axs[1].set_xlabel('Time')\n",
    "    axs[1].set_ylabel(target)\n",
    "    axs[1].legend()\n",
    "    \n",
    "    fig.suptitle(f'{target} - Actual vs Predicted')\n",
    "    plt.savefig(os.path.join(save_directory, f'{target}_yy_vs_time_{eval_metric}.png'))\n",
    "    plt.close(fig)\n",
    "\n",
    "    # 残差 vs. 説明変数\n",
    "    fig, ax = plt.subplots(1, len(features), figsize=(10*len(features), 20))\n",
    "    for i in range(len(features)):\n",
    "        if len(features) > 1:\n",
    "            ax[i].scatter(train_df[features[i]], residuals)\n",
    "            ax[i].set_xlabel(features[i])\n",
    "            ax[i].set_ylabel('Residuals')\n",
    "        else:\n",
    "            ax.scatter(train_df[features[i]], residuals)\n",
    "            ax.set_xlabel(features[i])\n",
    "            ax.set_ylabel('Residuals')\n",
    "    plt.savefig(os.path.join(save_directory,f'{target}_residuals_vs_xvars_{eval_metric}.png'))\n",
    "    plt.close(fig)\n",
    "\n",
    "def predict_and_evaluate(models, df, target_to_features, save_path):\n",
    "    \"\"\"\n",
    "    与えられたモデルを使って、指定されたDataFrameに対して予測を行い、各種評価指標とグラフを表示します。\n",
    "    Args:\n",
    "        models (dict): ターゲット変数ごとに訓練されたモデルの辞書\n",
    "        df (pd.DataFrame): 予測を行う対象のデータフレーム\n",
    "        target_to_features (dict): ターゲット変数ごとに使用する説明変数のリストを格納した辞書, 説明変数にはOne Hot Encodingされて生成された時間列含む。\n",
    "        save_path (str): グラフを保存するディレクトリのパス\n",
    "    \"\"\"\n",
    "    # 時間（Hour）列を作成\n",
    "    df = add_hour_column(df)\n",
    "    \n",
    "    # 時間列に対してOne Hot Encodingを実行\n",
    "    df = one_hot_encode_hours(df)\n",
    "\n",
    "    # 結果を格納するDataFrameを作成\n",
    "    result_df = pd.DataFrame(columns=['y_var', 'mse', 'mae', 'r2'])\n",
    "\n",
    "    for target, features in target_to_features.items():\n",
    "        # 予測値を計算\n",
    "        y_pred = models[target].predict(df[features])\n",
    "\n",
    "        # 評価指標を計算\n",
    "        mse = mean_squared_error(df[target], y_pred)\n",
    "        mae = mean_absolute_error(df[target], y_pred)\n",
    "        r2 = r2_score(df[target], y_pred)\n",
    "\n",
    "        # 結果をDataFrameに追加\n",
    "        result_df.loc[len(result_df)] = [target, mse, mae, r2]\n",
    "\n",
    "        # グラフの作成\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "        ax.plot(df['time'], df[target], label='Actual')\n",
    "        ax.plot(df['time'], y_pred, label='Predicted')\n",
    "        ax.set_title(f'{target} - Actual vs Predicted')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel(target)\n",
    "        ax.legend()\n",
    "\n",
    "        # グラフの保存\n",
    "        plt.savefig(os.path.join(save_path, f'{target}_prediction.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "    # 結果の表示\n",
    "    print(result_df)\n",
    "\n",
    "    # 結果の保存\n",
    "    result_file_path = os.path.join(save_path, 'prediction_result.csv')\n",
    "    result_df.to_csv(result_file_path, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 勾配ブースティング(XGBRegressor)のパラメータ探索グリッドサーチ関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_regression_and_analysis_grid(train_df, test_df, var_dict, param_grid, eval_metric, scoring, save_path):\n",
    "    \"\"\"\n",
    "    説明変数と目的変数を指定して、XGBoostingによる機械学習を実行し、予測値を計算する関数。\n",
    "    :param df: データフレーム。説明変数と目的変数が含まれる。\n",
    "    :type df: pandas.DataFrame\n",
    "    :param var_dict: 説明変数と目的変数の辞書。キーが目的変数名で、値が説明変数名のリスト。\n",
    "    :type var_dict: dict of str to list of str\n",
    "    :param save_path: グラフを保存するフォルダのパス。\n",
    "    :type save_path: str\n",
    "    :return: 構築したモデルと予測値の辞書\n",
    "    :rtype: tuple of dict\n",
    "    \"\"\"\n",
    "    # 入力検証\n",
    "    validate_input(var_dict,train_df)\n",
    "    validate_input(var_dict,test_df)\n",
    "\n",
    "    # 指定されたパスにフォルダがなければ作成\n",
    "    create_folder(save_path)\n",
    "\n",
    "    models = {}\n",
    "    train_y_preds = {}\n",
    "    test_y_preds = {}\n",
    "\n",
    "    # 結果を格納するDataFrame作成\n",
    "    result_df = pd.DataFrame(columns=['y_var', 'train_mse', 'train_mae', 'train_r2', 'test_mse', 'test_mae', 'test_r2'])\n",
    "\n",
    "    for y_var, x_vars in var_dict.items():\n",
    "        \n",
    "        eval_set = [(train_df[x_vars], train_df[y_var])]\n",
    "        # モデルインスタンス作成\n",
    "        model = XGBRegressor(n_estimators=100, early_stopping_rounds=15, eval_set=eval_set, eval_metric=eval_metric)\n",
    "        model.fit(train_df[x_vars], train_df[y_var],  verbose=False)\n",
    "\n",
    "        # グリッドサーチ実行\n",
    "        grid_search = GridSearchCV(estimator=model,\n",
    "                                   param_grid=param_grid,\n",
    "                                   cv=3,\n",
    "                                   scoring=scoring,\n",
    "                                   n_jobs=-1)\n",
    "        grid_search.fit(train_df[x_vars], train_df[y_var], eval_set=eval_set, eval_metric=eval_metric, verbose=False)\n",
    "\n",
    "        # 最適なパラメータで再学習\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        models[y_var] = best_model\n",
    "        best_params_df = pd.DataFrame.from_dict(best_params, orient='index', columns=['value'])\n",
    "        best_params_file_path = os.path.join(save_path, f'best_params_{y_var}.csv')\n",
    "        best_params_df.to_csv(best_params_file_path)\n",
    "\n",
    "        # 学習用データでの予測と評価指標計算\n",
    "        train_y_pred = best_model.predict(train_df[x_vars])\n",
    "        train_y_preds[y_var] = train_y_pred\n",
    "        train_mse = mean_squared_error(train_df[y_var], train_y_pred)\n",
    "        train_mae = mean_absolute_error(train_df[y_var], train_y_pred)\n",
    "        train_r2 = r2_score(train_df[y_var], train_y_pred)\n",
    "        \n",
    "        # 検証用データでの予測と評価指標計算\n",
    "        test_y_pred = best_model.predict(test_df[x_vars])\n",
    "        test_y_preds[y_var] = test_y_pred\n",
    "        test_mse = mean_squared_error(test_df[y_var], test_y_pred)\n",
    "        test_mae = mean_absolute_error(test_df[y_var], test_y_pred)\n",
    "        test_r2 = r2_score(test_df[y_var], test_y_pred)\n",
    "\n",
    "        # 結果のサマリ表示\n",
    "        \"\"\"\n",
    "        print(f'{y_var} の学習用MSE:', train_mse)\n",
    "        print(f'{y_var} の学習用MAE:', train_mae)\n",
    "        print(f'{y_var} の学習用決定係数:', train_r2)\n",
    "\n",
    "        print(f'{y_var} の検証用MSE:', test_mse)\n",
    "        print(f'{y_var} の検証用MAE:', test_mae)\n",
    "        print(f'{y_var} の検証用決定係数:', test_r2)\n",
    "        print('\\n')\n",
    "        \"\"\"\n",
    "        \n",
    "        # 結果をDataFrameに追加\n",
    "        result_df.loc[len(result_df)] = [y_var, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2]\n",
    "\n",
    "        # 残差計算\n",
    "        residuals = train_df[y_var] - train_y_pred\n",
    "\n",
    "        # 標準化残差計算\n",
    "        standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "\n",
    "\n",
    "        # グラフのプロット部\n",
    "        # 観測値、予測値 vs. time\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(50, 16))\n",
    "        axs[0].plot(train_df['time'], train_df[y_var], label='Actual')\n",
    "        axs[0].plot(train_df['time'], train_y_pred, label='Predicted')\n",
    "        axs[0].set_title('Train Data')\n",
    "        axs[0].set_xlabel('Time')\n",
    "        axs[0].set_ylabel(y_var)\n",
    "        axs[0].legend()\n",
    "        \n",
    "        axs[1].plot(test_df['time'], test_df[y_var], label='Actual')\n",
    "        axs[1].plot(test_df['time'], test_y_pred, label='Predicted')\n",
    "        axs[1].set_title('Test Data')\n",
    "        axs[1].set_xlabel('Time')\n",
    "        axs[1].set_ylabel(y_var)\n",
    "        axs[1].legend()\n",
    "        \n",
    "        fig.suptitle(f'{y_var} - Actual vs Predicted')\n",
    "        plt.savefig(os.path.join(save_path, f'{y_var}_yy_vs_time_{eval_metric}.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # 残差 vs. 説明変数\n",
    "        fig, ax = plt.subplots(1, len(x_vars), figsize=(10*len(x_vars), 20))\n",
    "        for i in range(len(x_vars)):\n",
    "            if len(x_vars) > 1:\n",
    "                ax[i].scatter(train_df[x_vars[i]], residuals)\n",
    "                ax[i].set_xlabel(x_vars[i])\n",
    "                ax[i].set_ylabel('Residuals')\n",
    "            else:\n",
    "                ax.scatter(train_df[x_vars[i]], residuals)\n",
    "                ax.set_xlabel(x_vars[i])\n",
    "                ax.set_ylabel('Residuals')\n",
    "        plt.savefig(os.path.join(save_path,f'{y_var}_residuals_vs_xvars_{eval_metric}.png'))\n",
    "        plt.close(fig)\n",
    "        del train_y_pred, test_y_pred\n",
    "        gc.collect()\n",
    "\n",
    "    result_file_path = os.path.join(save_path, f'result_{eval_metric}.csv')\n",
    "    result_df.to_csv(result_file_path,index=False)\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train_df = df.loc[(df.index >= '2022-12-1 00:00:00') & (df.index < '2023-2-1 00:00:00')].reset_index()\n",
    "test_df = df.loc[(df.index >= '2022-11-01 00:00:00') & (df.index < '2022-12-1 00:00:00')].reset_index()\n",
    "\n",
    "var_dict = {\n",
    "    '3F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F':['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all':['ipa', 'sto', 'han', 'div', 'col', 'alt']}\n",
    "\n",
    "# グリッドサーチ用パラメータ設定\n",
    "param_grid = {\n",
    "    'learning_ration': [0.01, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 2, 4, 6, 8],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'colsample_bytree': [0.2, 0.5, 0.8, 1.0],\n",
    "    'subsample': [0.2, 0.6, 1.0]\n",
    "    }\n",
    "\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "eval_metric = 'mae'\n",
    "\n",
    "save_path = \"/workspaces/data/flow/xgboost_grid_param12-1\"\n",
    "\n",
    "# 時間(Hour)列を作成する\n",
    "train_df = extract_hour(train_df)\n",
    "test_df = extract_hour(test_df)\n",
    "\n",
    "# 時間列に対してOne Hot Encodingを実行する\n",
    "train_df = one_hot_encode_hour(train_df)\n",
    "test_df = one_hot_encode_hour(test_df)\n",
    "\n",
    "# 1から23までの整数のリストを作成\n",
    "hours = list(range(1, 24))\n",
    "\n",
    "# hour_1, hour_2, ..., hour_23 の形式のダミー変数名のリストを作成\n",
    "hour_dummies = [f'hour_{hour}' for hour in hours]\n",
    "\n",
    "# hourのダミー変数を追加した新しいvar_dict\n",
    "updated_var_dict = {}\n",
    "for target, features in var_dict.items():\n",
    "    updated_features = features + hour_dummies\n",
    "    updated_var_dict[target] = updated_features\n",
    "\n",
    "models = run_xgb_regression_and_analysis_grid(train_df, test_df, updated_var_dict, param_grid, eval_metric, scoring, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 勾配ブースティング(XGBRegressor)の調整済みパラメータによるフィッティング関数と交差検証関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_analyze_xgb_regression(train_df, test_df, target_to_features, params_directory, eval_metric, scoring, save_directory):\n",
    "    \"\"\"\n",
    "    XGBoost回帰モデルを学習し、予測と分析を行う関数。\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): 学習用データセット。\n",
    "        test_df (pd.DataFrame): 検証用データセット。\n",
    "        target_to_features (dict): 目的変数をキー、説明変数のリストを値とした辞書。\n",
    "        params_directory (str): ハイパーパラメータが保存されたディレクトリへのパス。\n",
    "        eval_metric (str): 評価指標。\n",
    "        scoring (str): スコアリング方法。\n",
    "        save_directory (str): 結果を保存するディレクトリへのパス。\n",
    "    Returns:\n",
    "        dict: 学習済みモデルを格納した辞書。\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate inputs\n",
    "    validate_input(target_to_features, train_df)\n",
    "    validate_input(target_to_features, test_df)\n",
    "\n",
    "    # Create the specified directory if it does not exist\n",
    "    create_directory_if_not_exists(save_directory)\n",
    "\n",
    "    models = {}\n",
    "    train_y_preds = {}\n",
    "    test_y_preds = {}\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    result_df = pd.DataFrame(columns=['y_var', 'train_mse', 'train_mae', 'train_r2', 'test_mse', 'test_mae', 'test_r2'])\n",
    "\n",
    "    for target, features in target_to_features.items():\n",
    "        \n",
    "        eval_set = [(train_df[features], train_df[target])]\n",
    "\n",
    "        # Load hyperparameters\n",
    "        hyperparams = load_hyperparameters_from_csv(params_directory, target)\n",
    "\n",
    "        # Create model parameters\n",
    "        model_params = {\n",
    "            'n_estimators': 100,\n",
    "            'early_stopping_rounds': 15,\n",
    "            'eval_set': eval_set,\n",
    "            'eval_metric': eval_metric,\n",
    "            'verbose': False\n",
    "        }\n",
    "\n",
    "        # Update model parameters with hyperparameters from the CSV file\n",
    "        model_params.update(hyperparams)\n",
    "\n",
    "        # Create a model instance\n",
    "        model = XGBRegressor(**model_params)        \n",
    "        model.fit(train_df[features], train_df[target], eval_set=eval_set)\n",
    "\n",
    "        models[target] = model\n",
    "\n",
    "        # Predict and calculate evaluation metrics for the training data\n",
    "        train_y_pred = model.predict(train_df[features])\n",
    "        train_y_preds[target] = train_y_pred\n",
    "        train_mse = mean_squared_error(train_df[target], train_y_pred)\n",
    "        train_mae = mean_absolute_error(train_df[target], train_y_pred)\n",
    "        train_r2 = r2_score(train_df[target], train_y_pred)\n",
    "        \n",
    "        # Predict and calculate evaluation metrics for the test data\n",
    "        test_y_pred = model.predict(test_df[features])\n",
    "        test_y_preds[target] = test_y_pred\n",
    "        test_mse = mean_squared_error(test_df[target], test_y_pred)\n",
    "        test_mae = mean_absolute_error(test_df[target], test_y_pred)\n",
    "        test_r2 = r2_score(test_df[target], test_y_pred)\n",
    "        \n",
    "        # Add the results to the DataFrame\n",
    "        result_df.loc[len(result_df)] = [target, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2]\n",
    "\n",
    "        # Calculate residuals\n",
    "        residuals = train_df[target] - train_y_pred\n",
    "\n",
    "        # Calculate standardized residuals\n",
    "        standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "\n",
    "        # Plot the graphs\n",
    "        plot_actual_vs_predicted_and_residuals(train_df, test_df, target, features, train_y_pred, test_y_pred, residuals, save_directory, eval_metric)\n",
    "        del train_y_pred, test_y_pred\n",
    "        gc.collect()\n",
    "\n",
    "    result_file_path = os.path.join(save_directory, f'result_{eval_metric}.csv')\n",
    "    result_df.to_csv(result_file_path, index=False)\n",
    "    pkl_file_path = os.path.join(save_directory, f'models_{eval_metric}.pkl')\n",
    "    pickle_dump(models, pkl_file_path)\n",
    "    return models\n",
    "\n",
    "def perform_xgb_cross_validation(df, updated_target_to_features, params_directory, eval_metric, scoring, save_directory, n_splits):\n",
    "    \"\"\"\n",
    "    クロスバリデーションによるXGBoost回帰モデルの学習と評価を行う関数。\n",
    "    Args:\n",
    "        df (pd.DataFrame): 全データセット。\n",
    "        updated_target_to_features (dict): 目的変数をキー、説明変数のリストを値とした辞書。ダミー変数が更新されたもの。\n",
    "        params_directory (str): ハイパーパラメータが保存されたディレクトリへのパス。\n",
    "        eval_metric (str): 評価指標。\n",
    "        scoring (str): スコアリング方法。\n",
    "        save_directory (str): 結果を保存するディレクトリへのパス。\n",
    "    Return:\n",
    "        dict: 学習済みモデルを格納した辞書。\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits)\n",
    "    models = {}\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(df)):\n",
    "        train_df = df.iloc[train_index]\n",
    "        test_df = df.iloc[test_index]\n",
    "        models.update(train_and_analyze_xgb_regression(train_df, test_df, updated_target_to_features, params_directory, eval_metric, scoring, f'{save_directory}/{i}'))\n",
    "        pkl_file_path = os.path.join(save_directory, f'models_cv{n_splits}_{eval_metric}.pkl')\n",
    "        pickle_dump(models, pkl_file_path)\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_t = df.loc[(df.index >= '2022-11-1 00:00:00') & (df.index < '2023-3-30 00:00:00')].reset_index()\n",
    "\n",
    "# scoring = 'neg_mean_absolute_error'\n",
    "# eval_metric = 'mae'\n",
    "# scoring = 'r2'\n",
    "# eval_metric = 'rmse'\n",
    "scoring = 'neg_mean_squared_error'\n",
    "eval_metric = 'rmse'\n",
    "\n",
    "n_sprits = 5\n",
    "\n",
    "params_path = \"/workspaces/data/flow/best_params/\"\n",
    "save_directory = f\"/workspaces/data/flow/xgboost_crossvalidation_cv{n_sprits}_{eval_metric}\"\n",
    "\n",
    "# Create an 'hour' column\n",
    "df_t = add_hour_column(df_t)\n",
    "\n",
    "# Perform one-hot encoding on the 'hour' column\n",
    "df_t = one_hot_encode_hours(df_t)\n",
    "\n",
    "# Create a list of integers from 1 to 23\n",
    "hours = list(range(1, 24))\n",
    "\n",
    "# Create a list of dummy variable names in the format hour_1, hour_2, ..., hour_23\n",
    "hour_dummies = [f'hour_{hour}' for hour in hours]\n",
    "\n",
    "var_dict = {\n",
    "    '3F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all': ['ipa', 'sto', 'han', 'div', 'col', 'alt']\n",
    "}\n",
    "\n",
    "# Create a new var_dict with the 'hour' dummy variables added\n",
    "updated_var_dict = {}\n",
    "for target, features in var_dict.items():\n",
    "    updated_features = features + hour_dummies\n",
    "    updated_var_dict[target] = updated_features\n",
    "\n",
    "models = perform_xgb_cross_validation(df_t, updated_var_dict, params_path, eval_metric, scoring, save_directory, n_sprits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 複数のモデルからベストなモデルを選定する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(models_directory, df, target_to_features, scoring, save_directory):\n",
    "    # 時間（Hour）列を作成\n",
    "    df = add_hour_column(df)\n",
    "    \n",
    "    # 時間列に対してOne Hot Encodingを実行\n",
    "    df = one_hot_encode_hours(df)\n",
    "\n",
    "    # Create the specified directory if it does not exist\n",
    "    create_directory_if_not_exists(save_directory)\n",
    "\n",
    "    best_scores = {}\n",
    "    best_models = {}\n",
    "    best_model_paths = {}\n",
    "\n",
    "    # Scoring functions\n",
    "    scoring_functions = {\n",
    "        'mse': (mean_squared_error, float('inf')),\n",
    "        'mae': (mean_absolute_error, float('inf')),\n",
    "        'r2': (r2_score, float('-inf')),\n",
    "        'neg_mean_absolute_error': (mean_absolute_error, float('inf')),\n",
    "    }\n",
    "\n",
    "    if scoring not in scoring_functions:\n",
    "        raise ValueError(f\"Invalid scoring method '{scoring}'. Supported methods are {list(scoring_functions.keys())}\")\n",
    "\n",
    "    score_function, initial_score = scoring_functions[scoring]\n",
    "\n",
    "    for model_file in os.listdir(models_directory):\n",
    "        if model_file.endswith(\".pkl\"):\n",
    "            file_path = os.path.join(models_directory, model_file)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                models = pickle.load(file)\n",
    "            for target, model in models.items():\n",
    "                features = target_to_features[target]\n",
    "                y_true = df[target]\n",
    "                y_pred = model.predict(df[features])\n",
    "                score = score_function(y_true, y_pred)\n",
    "                # print(f\"Model: {model_file}, Score: {score}\")\n",
    "                if target not in best_scores or (scoring == 'r2' and score > best_scores[target]) or (scoring == 'mae' and score < best_scores[target]):\n",
    "                    best_scores[target] = score\n",
    "                    best_models[target] = model\n",
    "                    best_model_paths[target] = file_path\n",
    "    pkl_file_path = os.path.join(save_directory, f'best_models_{scoring}.pkl')\n",
    "    pickle_dump(best_models, pkl_file_path)\n",
    "    return best_models, best_scores, best_model_paths\n",
    "\n",
    "models_directory = '/workspaces/data/flow/models/'\n",
    "result_directory = '/workspaces/data/flow/bestmodels/'\n",
    "\n",
    "# Load the data\n",
    "df_t = df.loc[(df.index >= '2022-11-6 00:00:00') & (df.index < '2023-1-1 00:00:00')].reset_index()\n",
    "\n",
    "var_dict = {\n",
    "    '3F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all': ['ipa', 'sto', 'han', 'div', 'col', 'alt']\n",
    "}\n",
    "\n",
    "# Create a list of integers from 1 to 23\n",
    "hours = list(range(1, 24))\n",
    "\n",
    "# Create a list of dummy variable names in the format hour_1, hour_2, ..., hour_23\n",
    "hour_dummies = [f'hour_{hour}' for hour in hours]\n",
    "\n",
    "# Create a new var_dict with the 'hour' dummy variables added\n",
    "updated_var_dict = {}\n",
    "for target, features in var_dict.items():\n",
    "    updated_features = features + hour_dummies\n",
    "    updated_var_dict[target] = updated_features\n",
    "\n",
    "scoring_list = ['r2', 'mse', 'mae']\n",
    "\n",
    "for scoring in scoring_list:\n",
    "    _, result, path = select_best_model(models_directory, df_t, updated_var_dict, scoring, result_directory)\n",
    "    print(scoring)\n",
    "    print(result)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_directory = '/workspaces/data/flow/models/'\n",
    "\n",
    "# Load the data\n",
    "df_t = df.loc[(df.index >= '2022-11-6 00:00:00') & (df.index < '2023-1-1 00:00:00')].reset_index()\n",
    "\n",
    "# Create a list of integers from 1 to 23\n",
    "hours = list(range(1, 24))\n",
    "\n",
    "# Create a list of dummy variable names in the format hour_1, hour_2, ..., hour_23\n",
    "hour_dummies = [f'hour_{hour}' for hour in hours]\n",
    "\n",
    "var_dict = {\n",
    "    '3F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all': ['ipa', 'sto', 'han', 'div', 'col', 'alt']\n",
    "}\n",
    "\n",
    "# Create a new var_dict with the 'hour' dummy variables added\n",
    "updated_var_dict = {}\n",
    "for target, features in var_dict.items():\n",
    "    updated_features = features + hour_dummies\n",
    "    updated_var_dict[target] = updated_features\n",
    "\n",
    "list = ['r2', 'mse', 'mae']\n",
    "\n",
    "for scoring in list:\n",
    "    _, result, path = select_best_model(models_directory, df_t, updated_var_dict, scoring)\n",
    "    print(scoring)\n",
    "    print(result)\n",
    "    print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluate_values_models(models_directory, df, target_to_features, save_directory):\n",
    "    \"\"\"\n",
    "    構築済みのモデルを評価する関数。\n",
    "    :param models_file_path: 各目的変数のモデルが格納された辞書型変数が格納されたpklファイルへのpath\n",
    "    :type models_file_path: the path to pkl file of dict{key:str(target), value:model}\n",
    "    :param df: 元データ\n",
    "    :type df: pd.DataFrame\n",
    "    :param target_to_features: 説明変数と目的変数の辞書。キーが目的変数名で、値が説明変数名のリスト。\n",
    "    :type target_to_features: dict of str to list of str\n",
    "    \"\"\"\n",
    "    # 時間（Hour）列を作成\n",
    "    df = add_hour_column(df)\n",
    "    \n",
    "    # 時間列に対してOne Hot Encodingを実行\n",
    "    df = one_hot_encode_hours(df)\n",
    "\n",
    "    # Scoring functions\n",
    "    scoring_functions = {\n",
    "        'mse': (mean_squared_error, float('inf')),\n",
    "        'mae': (mean_absolute_error, float('inf')),\n",
    "        'r2': (r2_score, float('-inf')),\n",
    "    }\n",
    "\n",
    "    columns = ['model_file']\n",
    "    for target in target_to_features.keys():\n",
    "        for scoring in scoring_functions.keys():\n",
    "            columns.append(f'{target}_{scoring}')\n",
    "    scores_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # 各モデルファイルに対してスコアを計算し、DataFrameに追加します。\n",
    "    for model_file in os.listdir(models_directory):\n",
    "        if model_file.endswith(\".pkl\"):\n",
    "            file_path = os.path.join(models_directory, model_file)\n",
    "            models = pickle_load(file_path)\n",
    "            row = {'model_file': model_file}\n",
    "            for target, model in models.items():\n",
    "                features = target_to_features[target]\n",
    "                y_true = df[target]\n",
    "                y_pred = model.predict(df[features])\n",
    "                for scoring, scoring_function  in scoring_functions.items():\n",
    "                    score = scoring_function[0](y_true, y_pred)\n",
    "                    row[f'{target}_{scoring}'] = score\n",
    "            scores_df = scores_df.append(row, ignore_index=True)\n",
    "    scores_df.to_csv(f'{result_directory}result.csv')\n",
    "\n",
    "models_directory = '/workspaces/data/flow/bestmodels/'\n",
    "result_directory = '/workspaces/data/flow/result/'\n",
    "\n",
    "# Load the data\n",
    "df_t = df.loc[(df.index >= '2022-12-1 00:00:00') & (df.index < '2023-11-1 00:00:00')].reset_index()\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Create a list of integers from 1 to 23\n",
    "hours = list(range(1, 24))\n",
    "\n",
    "# Create a list of dummy variable names in the format hour_1, hour_2, ..., hour_23\n",
    "hour_dummies = [f'hour_{hour}' for hour in hours]\n",
    "\n",
    "var_dict = {\n",
    "    '3F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '2F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    '1F': ['ipa', 'sto', 'han', 'div', 'col', 'alt'],\n",
    "    'all': ['ipa', 'sto', 'han', 'div', 'col', 'alt']\n",
    "}\n",
    "\n",
    "# Create a new var_dict with the 'hour' dummy variables added\n",
    "updated_var_dict = {}\n",
    "for target, features in var_dict.items():\n",
    "    updated_features = features + hour_dummies\n",
    "    updated_var_dict[target] = updated_features\n",
    "\n",
    "print_evaluate_values_models(models_directory, df_t, updated_var_dict, result_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
